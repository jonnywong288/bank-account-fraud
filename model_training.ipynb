{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import math\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, precision_recall_curve, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "from python_scripts.model_performance import generate_df_summary, predict_max_f1, save_model\n",
    "# import feature data types\n",
    "import json\n",
    "with open('python_scripts/data_types.json') as f:\n",
    "    data_types = json.load(f) \n",
    "\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('data/X_train.parquet')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "X_val = pd.read_parquet('data/X_val.parquet')\n",
    "y_val = pd.read_csv('data/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "y_pred = baseline_model.predict(X_val)\n",
    "\n",
    "generate_df_summary(baseline_model, y_val, y_pred, 'baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(baseline_model, X_val, y_val)\n",
    "generate_df_summary(baseline_model, y_val, y_pred, 'baseline model threshold optimised', threshold=best_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_10 = pd.read_parquet('data/resampled/X_train_10.parquet')\n",
    "y_train_resampled_10 = pd.read_parquet('data/resampled/y_train_10.parquet')\n",
    "\n",
    "X_train_resampled_25 = pd.read_parquet('data/resampled/X_train_25.parquet')\n",
    "y_train_resampled_25 = pd.read_parquet('data/resampled/y_train_25.parquet')\n",
    "\n",
    "X_train_resampled_50 = pd.read_parquet('data/resampled/X_train_50.parquet')\n",
    "y_train_resampled_50 = pd.read_parquet('data/resampled/y_train_50.parquet')\n",
    "\n",
    "X_train_resampled_100 = pd.read_parquet('data/resampled/X_train_100.parquet')\n",
    "y_train_resampled_100 = pd.read_parquet('data/resampled/y_train_100.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_10 = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "baseline_model_10.fit(X_train_resampled_10, y_train_resampled_10)\n",
    "y_pred = baseline_model_10.predict(X_val)\n",
    "generate_df_summary(baseline_model_10, y_val, y_pred, 'baseline model with 10:1 resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(baseline_model_10, X_val, y_val)\n",
    "generate_df_summary(baseline_model_10, y_val, y_pred, 'baseline model with 10:1 resampling threshold optimised', threshold=best_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_25 = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "baseline_model_25.fit(X_train_resampled_25, y_train_resampled_25)\n",
    "y_pred = baseline_model_25.predict(X_val)\n",
    "generate_df_summary(baseline_model_25, y_val, y_pred, 'baseline model with 4:1 resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(baseline_model_25, X_val, y_val)\n",
    "generate_df_summary(baseline_model_25, y_val, y_pred, 'baseline model with 4:1 resampling threshold optimised', threshold=best_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_50 = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "baseline_model_50.fit(X_train_resampled_50, y_train_resampled_50)\n",
    "y_pred = baseline_model_50.predict(X_val)\n",
    "generate_df_summary(baseline_model_50, y_val, y_pred, 'baseline model with 2:1 resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(baseline_model_50, X_val, y_val)\n",
    "generate_df_summary(baseline_model_50, y_val, y_pred, 'baseline model with 2:1 resampling threshold optimised', threshold=best_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_100 = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "baseline_model_100.fit(X_train_resampled_100, y_train_resampled_100)\n",
    "y_pred = baseline_model_100.predict(X_val)\n",
    "generate_df_summary(baseline_model_50, y_val, y_pred, 'baseline model with 1:1 resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(baseline_model_100, X_val, y_val)\n",
    "generate_df_summary(baseline_model_100, y_val, y_pred, 'baseline model with 1:1 resampling threshold optimised', threshold=best_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = pd.read_csv('output/model_performance.csv')\n",
    "\n",
    "ratios = ['Original', '10:1', '4:1', '2:1', '1:1']\n",
    "f1s = mp_df['F1-Score'][::2].values[:5]\n",
    "f1s_optimised = mp_df['F1-Score'][1::2].values[:5]\n",
    "\n",
    "bar_width = 0.35  \n",
    "x = np.arange(len(ratios)) \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x - bar_width/2, f1s, bar_width, label='threshold=0.5', color='blue')\n",
    "plt.bar(x + bar_width/2, f1s_optimised, bar_width, label='f1 optimised', color='orange')\n",
    "plt.xlabel('Resampling Ratio')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Baseline f1-scores on different resampling ratios.')\n",
    "plt.xticks(ticks=x, labels=ratios) \n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive with more combinations and WITHOUT smote, using xgboost inbuild scale_pos_weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# inital param search\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],  \n",
    "    'max_depth': [3, 4, 5, 6, 8, 10], \n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'reg_lambda': [0.1, 1, 10, 100], \n",
    "    'reg_alpha': [0, 0.1, 1, 10],  \n",
    "    'scale_pos_weight': [0, 5, 10, 25, 50],  \n",
    "    'max_delta_step': [0, 1, 5, 10],  \n",
    "    'gamma': [0, 0.1, 0.5, 1, 5],  \n",
    "    'subsample': [0.5, 0.7, 0.8, 1.0],  \n",
    "    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],  \n",
    "\n",
    "    'n_estimators': [1000],\n",
    "    'early_stopping_rounds': [15],\n",
    "}\n",
    "\n",
    "\n",
    "# RandomizedSearchCV setup for parameter tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100, \n",
    "    scoring='average_precision',  \n",
    "    cv=3,  \n",
    "    verbose=0,  \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Fit the model (train and tune hyperparameters)\n",
    "random_search.fit(X_train, y_train, \n",
    "                  eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Get the best model and print the results\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(best_model, X_val, y_val)\n",
    "generate_df_summary(best_model, y_val, y_pred, 'random_search_99:1', threshold=best_threshold)\n",
    "save_model(random_search, 'saved_models/random_search_99:1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (train and tune hyperparameters)\n",
    "random_search.fit(X_train_resampled_10, y_train_resampled_10, \n",
    "                  eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Get the best model and print the results\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(best_model, X_val, y_val)\n",
    "generate_df_summary(best_model, y_val, y_pred, 'random_search_10:1', threshold=best_threshold)\n",
    "save_model(random_search, \"saved_models/random_search_10:1.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (train and tune hyperparameters)\n",
    "random_search.fit(X_train_resampled_25, y_train_resampled_25, \n",
    "                  eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Get the best model and print the results\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(best_model, X_val, y_val)\n",
    "generate_df_summary(best_model, y_val, y_pred, 'random_search_4:1', threshold=best_threshold)\n",
    "save_model(random_search, \"saved_models/random_search_4:1.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (train and tune hyperparameters)\n",
    "random_search.fit(X_train_resampled_50, y_train_resampled_50, \n",
    "                  eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Get the best model and print the results\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(best_model, X_val, y_val)\n",
    "generate_df_summary(best_model, y_val, y_pred, 'random_search_2:1', threshold=best_threshold)\n",
    "save_model(random_search, \"saved_models/random_search_2:1.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1:1 oversampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (train and tune hyperparameters)\n",
    "random_search.fit(X_train_resampled_100, y_train_resampled_100, \n",
    "                  eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Get the best model and print the results\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, best_threshold = predict_max_f1(best_model, X_val, y_val)\n",
    "generate_df_summary(best_model, y_val, y_pred, 'random_search_1:1', threshold=best_threshold)\n",
    "save_model(random_search, \"saved_models/random_search_1:1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance of the best performing models from each random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do best threshold test on each of the winning models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View impact of hyper parameters on model performance of winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = 'REPLACE STRING HERE' # <-----------------\n",
    "results_df = pd.read_csv(f'saved_models/{best_model}/results.csv')\n",
    "to_analyse = ['param_subsample', 'param_scale_pos_weight',\n",
    "       'param_reg_lambda', 'param_reg_alpha',\n",
    "       'param_min_child_weight', 'param_max_depth', 'param_max_delta_step',\n",
    "       'param_learning_rate', 'param_gamma', 'param_colsample_bytree']\n",
    "\n",
    "# subplots\n",
    "num_vars = len(to_analyse)\n",
    "cols = 3 \n",
    "rows = math.ceil(num_vars / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(to_analyse):\n",
    "    sns.regplot(x=results_df[\"rank_test_score\"], \n",
    "                y=results_df[param], \n",
    "                order=2,  \n",
    "                scatter_kws={'alpha': 0.5},  \n",
    "                ax=axes[i])  # plot in respective subplot\n",
    "\n",
    "    # correlation\n",
    "    correlation = round(results_df[param].corr(results_df[\"rank_test_score\"]), 2)\n",
    "\n",
    "    axes[i].set_xlabel(\"Model Rank (Lower is Better)\")\n",
    "    axes[i].set_ylabel(param)\n",
    "    axes[i].set_title(f\"{param} vs. Model Rank (corr: {correlation})\")\n",
    "\n",
    "# remove spares subplts\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "fig.suptitle(\"Relationship Between Hyperparameters and Model Rank\", fontsize=25, y=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final random search just on the best weighting of oversampling with adjusted grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted grid\n",
    "# best oversampling ratio\n",
    "# save best model as the winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust winning model weights using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove early stopping\n",
    "# change number of boosting rounds\n",
    "# save final hyperparameters in a yaml file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model as pkl ready to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
